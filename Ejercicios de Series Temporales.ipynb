{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este script lee los datos del archivo train store.csv y en base a la venta, entrena 5 modelos diferentes de ML con diferentes hiperparametros\n",
    "Escoge el mejor modelo menor MSE y MAE y prueba los datos del archivo train.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo: Decision Tree\n",
      "Con los siguientes Hiperparametros: {'max_depth': 3}\n",
      "Con los siguientes Hiperparametros: {'max_depth': 5}\n",
      "Con los siguientes Hiperparametros: {'max_depth': 7}\n",
      "Entrenando modelo: KNN\n",
      "Con los siguientes Hiperparametros: {'n_neighbors': 3}\n",
      "Con los siguientes Hiperparametros: {'n_neighbors': 5}\n",
      "Con los siguientes Hiperparametros: {'n_neighbors': 7}\n",
      "Entrenando modelo: Linear Regression\n",
      "Con los siguientes Hiperparametros: {}\n",
      "Entrenando modelo: Random Forest\n",
      "Con los siguientes Hiperparametros: {'n_estimators': 50}\n",
      "Con los siguientes Hiperparametros: {'n_estimators': 100}\n",
      "Con los siguientes Hiperparametros: {'n_estimators': 150}\n",
      "Entrenando modelo: XGBoost\n",
      "Con los siguientes Hiperparametros: {'max_depth': 3, 'n_estimators': 50}\n",
      "Con los siguientes Hiperparametros: {'max_depth': 3, 'n_estimators': 100}\n",
      "Con los siguientes Hiperparametros: {'max_depth': 3, 'n_estimators': 150}\n",
      "Con los siguientes Hiperparametros: {'max_depth': 5, 'n_estimators': 50}\n",
      "Con los siguientes Hiperparametros: {'max_depth': 5, 'n_estimators': 100}\n",
      "Con los siguientes Hiperparametros: {'max_depth': 5, 'n_estimators': 150}\n",
      "Con los siguientes Hiperparametros: {'max_depth': 7, 'n_estimators': 50}\n",
      "Con los siguientes Hiperparametros: {'max_depth': 7, 'n_estimators': 100}\n",
      "Con los siguientes Hiperparametros: {'max_depth': 7, 'n_estimators': 150}\n",
      "                                              Modelo      Modelo Nombre  \\\n",
      "4  (XGBRegressor(base_score=None, booster=None, c...            XGBoost   \n",
      "3  ((DecisionTreeRegressor(max_features=1.0, rand...      Random Forest   \n",
      "2                              (LinearRegression(),)  Linear Regression   \n",
      "1              (KNeighborsRegressor(n_neighbors=7),)                KNN   \n",
      "0              (DecisionTreeRegressor(max_depth=7),)      Decision Tree   \n",
      "\n",
      "       RMSE       MAE                   Mejor Hiperparámetro  \\\n",
      "4  8.074650  6.253340  {'max_depth': 5, 'n_estimators': 100}   \n",
      "3  8.265707  6.339733                  {'n_estimators': 150}   \n",
      "2  8.828299  6.757163                                     {}   \n",
      "1  9.071497  6.925951                     {'n_neighbors': 7}   \n",
      "0  9.396165  7.188709                       {'max_depth': 7}   \n",
      "\n",
      "   Tiempo de Entrenamiento:  \n",
      "4                 71.115240  \n",
      "3               3019.575726  \n",
      "2                  5.388978  \n",
      "1                278.117325  \n",
      "0                 10.056824  \n",
      "El mejor modelo es XGBoost, tiene los hiperparametros {'max_depth': 5, 'n_estimators': 100}, y con el archivo test tiene un rmse 7.7819938399441515 y un mae de 5.995464102949927\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, ParameterGrid\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import time\n",
    "\n",
    "# Definir los modelos con los hiperparámetros que deseas ajustar\n",
    "modelos = {\n",
    "    'Decision Tree': DecisionTreeRegressor(),\n",
    "    'KNN': KNeighborsRegressor(),\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(),\n",
    "    'XGBoost': XGBRegressor()\n",
    "}\n",
    "\n",
    "# Definir los hiperparámetros que deseas ajustar para cada modelo\n",
    "hiperparametros = {\n",
    "    'Decision Tree': {'max_depth': [3, 5, 7]},\n",
    "    'KNN': {'n_neighbors': [3, 5, 7]},\n",
    "    'Linear Regression': {},\n",
    "    'Random Forest': {'n_estimators': [50, 100, 150]},\n",
    "    'XGBoost': {'n_estimators': [50, 100, 150], 'max_depth': [3, 5, 7]}\n",
    "}\n",
    "\n",
    "# Leer datos y convertir a tipo fecha\n",
    "df = pd.read_csv(\"train store.csv\")\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# ordenar por fecha\n",
    "df = df.sort_values(by=['date', 'store', 'item'])\n",
    "\n",
    "# Agregar columnas de día, mes y año\n",
    "df['day'] = df['date'].dt.day\n",
    "df['day_of_week'] = df['date'].dt.dayofweek\n",
    "df['month'] = df['date'].dt.month\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "# Calcular 7 lags temporales y eliminar nulos \n",
    "max_lags = 7\n",
    "for i in range(1, max_lags + 1):\n",
    "    df[f'sales_daylag{i}'] = df.groupby(['store', 'item'])['sales'].shift(i)\n",
    "df.dropna(inplace=True)\n",
    "df = df.drop(['date'], axis=1)\n",
    "\n",
    "# Convertir datos categoricos a numericos\n",
    "categorical_cols = ['store', 'item']\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "encoded_cols = encoder.fit_transform(df[categorical_cols])\n",
    "encoded_df = pd.DataFrame(encoded_cols, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "df = df.drop(columns=categorical_cols)\n",
    "df = pd.concat([df, encoded_df], axis=1)\n",
    "df = df.dropna()\n",
    "\n",
    "# Crear splits temporales para evaluacion de los modelos\n",
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Inicializar lista de resultados\n",
    "resultados = []\n",
    "\n",
    "# Iterar sobre cada modelo\n",
    "for nombre_modelo, modelo in modelos.items():\n",
    "    print('Entrenando modelo:', nombre_modelo)\n",
    "    # Inicializar la mejor puntuación para este modelo con un valor grande\n",
    "    mejor_modelo = ''\n",
    "    mejor_modelo_nombre = ''\n",
    "    mejor_puntuacion = float('inf') \n",
    "\n",
    "    # Iterar sobre cada combinación de hiperparámetros\n",
    "    for params in ParameterGrid(hiperparametros[nombre_modelo]):\n",
    "        print('Con los siguientes Hiperparametros:', params)\n",
    "        # Configurar el modelo con los hiperparámetros actuales\n",
    "        modelo.set_params(**params)\n",
    "\n",
    "        # Inicializar listas para guardar las puntuaciones de MSE y MAE para cada split\n",
    "        mse_scores = []\n",
    "        mae_scores = []\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Iterar sobre cada split temporal\n",
    "        for train_index, test_index in tscv.split(df):\n",
    "            # Dividir los datos en conjunto de entrenamiento y prueba\n",
    "            train = df.iloc[train_index]\n",
    "            test = df.iloc[test_index]\n",
    "\n",
    "            X_train = train.drop('sales', axis=1)\n",
    "            y_train = train['sales']\n",
    "            X_test = test.drop('sales', axis=1)\n",
    "            y_test = test['sales']\n",
    "\n",
    "            # Entrenar y evaluar el modelo\n",
    "            modelo.fit(X_train, y_train)\n",
    "            y_pred = modelo.predict(X_test)\n",
    "\n",
    "            mse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "            mse_scores.append(mse)\n",
    "            mae_scores.append(mae)\n",
    "\n",
    "        # Calcular la puntuación media de MSE y MAE para este modelo y estos hiperparámetros y el tiempo de ejecucion\n",
    "        train_time = time.time() - start_time\n",
    "        mse_mean = np.mean(mse_scores)\n",
    "        mae_mean = np.mean(mae_scores)\n",
    "\n",
    "        # Si encontramos una puntuación mejor, actualizar el mejor set de hiperparametros y la mejor puntuación\n",
    "        if mse_mean < mejor_puntuacion:\n",
    "            mejor_modelo = modelo\n",
    "            mejor_modelo_nombre = nombre_modelo\n",
    "            mejor_puntuacion = mse_mean\n",
    "            mejor_hiperparametros = params\n",
    "            mejor_tiempo = train_time\n",
    "\n",
    "    # Guardar en la lista de resultados el mejor set de hiperparametros por modelo\n",
    "    resultados.append({\n",
    "        'Modelo': mejor_modelo,\n",
    "        'Modelo Nombre': mejor_modelo_nombre, \n",
    "        'RMSE': mejor_puntuacion, \n",
    "        'MAE': mae_mean, \n",
    "        'Mejor Hiperparámetro': mejor_hiperparametros, \n",
    "        'Tiempo de Entrenamiento:': mejor_tiempo\n",
    "        })\n",
    "\n",
    "# Mostrar los resultados\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "\n",
    "# Ordenar los resultados por MSE y MAE en orden ascendente\n",
    "df_resultados = df_resultados.sort_values(by=['RMSE', 'MAE'])\n",
    "print(df_resultados)\n",
    "\n",
    "# Obtener el mejor modelo y sus hiperparámetros\n",
    "mejor_modelo_info = df_resultados.iloc[0]\n",
    "\n",
    "# Extraer el nombre del mejor modelo y sus hiperparámetros\n",
    "mejor_modelo_final = mejor_modelo_info['Modelo']\n",
    "mejor_modelo_nombre_final = mejor_modelo_info['Modelo Nombre']\n",
    "mejor_hiperparametros_final = mejor_modelo_info['Mejor Hiperparámetro']\n",
    "\n",
    "# Leer datos del archivo test y dar mismo trato que al train\n",
    "df_test = pd.read_csv(\"train store.csv\")  # AQUI FLTA PONER EL ARCHIVO DE TEST\n",
    "df_test['date'] = pd.to_datetime(df_test['date'])\n",
    "\n",
    "# ordenar por fecha\n",
    "df_test = df_test.sort_values(by=['date', 'store', 'item'])\n",
    "\n",
    "# Agregar columnas de día, mes y año\n",
    "df_test['day'] = df_test['date'].dt.day\n",
    "df_test['day_of_week'] = df_test['date'].dt.dayofweek\n",
    "df_test['month'] = df_test['date'].dt.month\n",
    "df_test['year'] = df_test['date'].dt.year\n",
    "\n",
    "# Calcular 7 lags temporales y eliminar nulos \n",
    "max_lags = 7\n",
    "for j in range(1, max_lags + 1):\n",
    "    df_test[f'sales_daylag{j}'] = df_test.groupby(['store', 'item'])['sales'].shift(j)\n",
    "df_test.dropna(inplace=True)\n",
    "df_test = df_test.drop(['date'], axis=1)\n",
    "\n",
    "# Convertir datos categoricos a numericos\n",
    "categorical_cols = ['store', 'item']\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "encoded_cols = encoder.fit_transform(df_test[categorical_cols])\n",
    "encoded_df_test = pd.DataFrame(encoded_cols, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "df_test = df_test.drop(columns=categorical_cols)\n",
    "df_test = pd.concat([df_test, encoded_df_test], axis=1)\n",
    "df_test = df_test.dropna()\n",
    "\n",
    "X_train_final = df.drop('sales', axis=1)\n",
    "y_train_final = df['sales']\n",
    "X_test_final = df_test.drop('sales', axis=1)\n",
    "y_test_final = df_test['sales']\n",
    "\n",
    "# Entrenar y evaluar el modelo\n",
    "modelo_final, = mejor_modelo_final  # Desempaqueta la tupla para obtener el modelo\n",
    "modelo_final.set_params(**mejor_hiperparametros_final)\n",
    "modelo_final.fit(X_train_final, y_train_final)\n",
    "y_pred_final = modelo_final.predict(X_test_final)\n",
    "\n",
    "mse_final = np.sqrt(mean_squared_error(y_test_final, y_pred_final))\n",
    "mae_final = mean_absolute_error(y_test_final, y_pred_final)\n",
    "\n",
    "print(f'El mejor modelo es {mejor_modelo_nombre_final}, tiene los hiperparametros {mejor_hiperparametros_final}, y con el archivo test tiene un rmse {mse_final} y un mae de {mae_final}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Feature shape mismatch, expected: 69, got 64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m modelo_final\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmejor_hiperparametros_final)\n\u001b[1;32m     57\u001b[0m modelo_final\u001b[38;5;241m.\u001b[39mfit(X_train_final, y_train_final)\n\u001b[0;32m---> 58\u001b[0m y_pred_final \u001b[38;5;241m=\u001b[39m modelo_final\u001b[38;5;241m.\u001b[39mpredict(df_test)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Desplegar resultados y mandar a un archivo CSV\u001b[39;00m\n\u001b[1;32m     61\u001b[0m resultado \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(y_pred_final)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/EAE/lib/python3.11/site-packages/xgboost/sklearn.py:1114\u001b[0m, in \u001b[0;36mXGBModel.predict\u001b[0;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_can_use_inplace_predict():\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1114\u001b[0m         predts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_booster()\u001b[38;5;241m.\u001b[39minplace_predict(\n\u001b[1;32m   1115\u001b[0m             data\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   1116\u001b[0m             iteration_range\u001b[38;5;241m=\u001b[39miteration_range,\n\u001b[1;32m   1117\u001b[0m             predict_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmargin\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_margin \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1118\u001b[0m             missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1119\u001b[0m             base_margin\u001b[38;5;241m=\u001b[39mbase_margin,\n\u001b[1;32m   1120\u001b[0m             validate_features\u001b[38;5;241m=\u001b[39mvalidate_features,\n\u001b[1;32m   1121\u001b[0m         )\n\u001b[1;32m   1122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_array(predts):\n\u001b[1;32m   1123\u001b[0m             \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupy\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=import-error\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/EAE/lib/python3.11/site-packages/xgboost/core.py:2269\u001b[0m, in \u001b[0;36mBooster.inplace_predict\u001b[0;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[1;32m   2265\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   2266\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`shape` attribute is required when `validate_features` is True.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2267\u001b[0m         )\n\u001b[1;32m   2268\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features() \u001b[38;5;241m!=\u001b[39m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m-> 2269\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2270\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature shape mismatch, expected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2271\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2272\u001b[0m         )\n\u001b[1;32m   2274\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m   2275\u001b[0m     _array_interface,\n\u001b[1;32m   2276\u001b[0m     _is_cudf_df,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2279\u001b[0m     _transform_pandas_df,\n\u001b[1;32m   2280\u001b[0m )\n\u001b[1;32m   2282\u001b[0m enable_categorical \u001b[38;5;241m=\u001b[39m _has_categorical(\u001b[38;5;28mself\u001b[39m, data)\n",
      "\u001b[0;31mValueError\u001b[0m: Feature shape mismatch, expected: 69, got 64"
     ]
    }
   ],
   "source": [
    "# Leer datos y convertir a tipo fecha\n",
    "df = pd.read_csv(\"train store.csv\")\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# ordenar por fecha\n",
    "df = df.sort_values(by=['date', 'store', 'item'])\n",
    "\n",
    "# Agregar columnas de día, mes y año\n",
    "df['day'] = df['date'].dt.day\n",
    "df['day_of_week'] = df['date'].dt.dayofweek\n",
    "df['month'] = df['date'].dt.month\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "# Calcular 7 lags temporales y eliminar nulos \n",
    "max_lags = 7\n",
    "for i in range(1, max_lags + 1):\n",
    "    df[f'sales_daylag{i}'] = df.groupby(['store', 'item'])['sales'].shift(i)\n",
    "df.dropna(inplace=True)\n",
    "df = df.drop(['date'], axis=1)\n",
    "\n",
    "# Convertir datos categoricos a numericos\n",
    "categorical_cols = ['store', 'item']\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "encoded_cols = encoder.fit_transform(df[categorical_cols])\n",
    "encoded_df = pd.DataFrame(encoded_cols, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "df = df.drop(columns=categorical_cols)\n",
    "df = pd.concat([df, encoded_df], axis=1)\n",
    "df = df.dropna()\n",
    "\n",
    "# Leer datos del archivo test y dar mismo trato que al train\n",
    "df_test = pd.read_csv(\"test.csv\")  \n",
    "df_test['date'] = pd.to_datetime(df_test['date'])\n",
    "\n",
    "# ordenar por fecha\n",
    "df_test = df_test.sort_values(by=['date', 'store', 'item'])\n",
    "\n",
    "# Agregar columnas de día, mes y año\n",
    "df_test['day'] = df_test['date'].dt.day\n",
    "df_test['day_of_week'] = df_test['date'].dt.dayofweek\n",
    "df_test['month'] = df_test['date'].dt.month\n",
    "df_test['year'] = df_test['date'].dt.year\n",
    "\n",
    "# Convertir datos categoricos a numericos\n",
    "categorical_cols = ['store', 'item']\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "encoded_cols = encoder.fit_transform(df_test[categorical_cols])\n",
    "encoded_df_test = pd.DataFrame(encoded_cols, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "df_test = df_test.drop(columns=categorical_cols)\n",
    "df_test = pd.concat([df_test, encoded_df_test], axis=1)\n",
    "df_test = df_test.dropna()\n",
    "\n",
    "# Entrenar modelo final\n",
    "X_train_final = df.drop('sales', axis=1)\n",
    "y_train_final = df['sales']\n",
    "modelo_final, = mejor_modelo_final  \n",
    "modelo_final.set_params(**mejor_hiperparametros_final)\n",
    "modelo_final.fit(X_train_final, y_train_final)\n",
    "y_pred_final = modelo_final.predict(df_test)\n",
    "\n",
    "# Desplegar resultados y mandar a un archivo CSV\n",
    "resultado = pd.DataFrame(y_pred_final)\n",
    "print(resultado)\n",
    "\n",
    "resultado.to_csv('resultado.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
